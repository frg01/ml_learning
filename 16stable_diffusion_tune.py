'''
pip install timm
pip install fairscale
pip install transformers
pip install requests
pip install accelerate
pip install diffusers
pip install einop
pip install safetensors
pip install voluptuous
pip install jax
pip install jaxlib
pip install peft
pip install deepface==0.0.92
pip install tensorflow==2.9.0  # ä¸ºäº†é¿å…æœ€åè¯„ä¼°é˜¶æ®µä½¿ç”¨deepfaceæ—¶çš„é”™è¯¯ï¼Œè¿™é‡Œé€‰æ‹©é™çº§ç‰ˆæœ¬
pip install keras
pip install opencv-python
'''
### å¯¼å…¥ ---------------------------------------------------------------
# ========== æ ‡å‡†åº“æ¨¡å— ==========
import os
import math
import glob
import shutil
import subprocess

# ========== ç¬¬ä¸‰æ–¹åº“ ==========
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm

# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========
from torchvision import transforms

# Transformers (Hugging Face)
from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor

# Diffusers (Hugging Face)
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    UNet2DConditionModel,
    DiffusionPipeline
)
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

# ========== LoRA æ¨¡å‹åº“ ==========
from peft import LoraConfig, get_peft_model, PeftModel

# ========== é¢éƒ¨æ£€æµ‹åº“ ==========
from deepface import DeepFace

import cv2


### è®¾ç½®é¡¹ç›®è·¯å¾„ ------------------------------------------------------------
# é¡¹ç›®åç§°å’Œæ•°æ®é›†åç§°
project_name = "Brad"
dataset_name = "Brad"

# æ ¹ç›®å½•å’Œä¸»è¦ç›®å½•
root_dir = "./"  # å½“å‰ç›®å½•
main_dir = os.path.join(root_dir, "SD")  # ä¸»ç›®å½•

# é¡¹ç›®ç›®å½•
project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•

# æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„
images_folder = os.path.join(main_dir, "Datasets", dataset_name)
prompts_folder = os.path.join(main_dir, "Datasets", "prompts")
captions_folder = images_folder  # ä¸åŸå§‹ä»£ç ä¸€è‡´
output_folder = os.path.join(project_dir, "logs")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹

# prompt æ–‡ä»¶è·¯å¾„
validation_prompt_name = "validation_prompt.txt"
validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)

# æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
model_path = os.path.join(project_dir, "logs", "checkpoint-last")

# å…¶ä»–è·¯å¾„è®¾ç½®
zip_file = os.path.join("./", "data/14/Datasets.zip")
inference_path = os.path.join(project_dir, "inference")  # ä¿å­˜æ¨ç†ç»“æœçš„æ–‡ä»¶å¤¹

os.makedirs(images_folder, exist_ok=True)
os.makedirs(prompts_folder, exist_ok=True)
os.makedirs(output_folder, exist_ok=True)
os.makedirs(inference_path, exist_ok=True)

# æ£€æŸ¥å¹¶è§£å‹æ•°æ®é›†
print("ğŸ“‚ æ­£åœ¨æ£€æŸ¥å¹¶è§£å‹æ ·ä¾‹æ•°æ®é›†...")

if not os.path.exists(zip_file):
    print("âŒ æœªæ‰¾åˆ°æ•°æ®é›†å‹ç¼©æ–‡ä»¶ Datasets.zipï¼")
    print("è¯·ä¸‹è½½æ•°æ®é›†:\n../Demos/data/14/Datasets.zip\nå¹¶æ”¾åœ¨ ./data/14 æ–‡ä»¶å¤¹ä¸‹")
else:
    subprocess.run(f"unzip -q -o {zip_file} -d {main_dir}", shell=True)
    print(f"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼")


### å¯¼å…¥æ•°æ® -----------------------------------------------------------
# è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡
resolution = 512

# æ•°æ®å¢å¼ºæ“ä½œ
train_transform = transforms.Compose(
    [
        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°
        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
    ]
)

### æ€ä¹ˆè®©æ¨¡å‹ç†è§£æ–‡æœ¬ CLIPTokenizer,CLIPï¼Œå…¨ç§° Contrastive Language-Image Pretrainingï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰
from transformers import CLITokenizer

# åˆå§‹åŒ– CLIPTokenizer
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# ç¤ºä¾‹ prompt
prompt_text = "A man in a graphic tee and sport coat."

# å…ˆä½¿ç”¨ tokenizer.tokenize æŸ¥çœ‹åˆ†è¯åçš„ token
tokens = tokenizer.tokenize(prompt_text)
print("Tokens:", tokens)

# å°†æ–‡æœ¬è½¬åŒ–ä¸º token
inputs = tokenizer(
    prompt_text,
    padding="max_length",  # å¦‚æœè¾“å…¥é•¿åº¦ä¸è¶³æœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œå¡«å……
    truncation=True,       # å¦‚æœè¾“å…¥è¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­
    return_tensors="pt"    # è¿”å› PyTorch å¼ é‡
)

# æ‰“å°åˆ†è¯åçš„ç»“æœ
print("Tokenized Input IDs:", inputs.input_ids)
print("Attention Mask:", inputs.attention_mask)


### è‡ªå®šä¹‰æ•°æ®é›† -------------------------------------------------------------------
"""
IMAGE_EXTENSIONSï¼šå®šä¹‰å¯æ¥å—çš„å›¾åƒæ–‡ä»¶æ‰©å±•ååˆ—è¡¨ã€‚
__init__ æ–¹æ³•ï¼š
    å›¾åƒè·¯å¾„ï¼šé€šè¿‡éå†æŒ‡å®šçš„å›¾åƒæ–‡ä»¶å¤¹ï¼Œè·å–æ‰€æœ‰ç¬¦åˆæ‰©å±•åçš„å›¾åƒæ–‡ä»¶è·¯å¾„ï¼Œå¹¶æ’åºã€‚
    æ–‡æœ¬æ ‡æ³¨ï¼šåœ¨æ ‡æ³¨æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æ‰€æœ‰ .txt æ–‡ä»¶ï¼Œè¯»å–å…¶å†…å®¹å¹¶å­˜å‚¨ä¸ºåˆ—è¡¨ã€‚
    ä¸€è‡´æ€§æ£€æŸ¥ï¼šç¡®ä¿å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´ã€‚
    æ–‡æœ¬ç¼–ç ï¼šä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º token IDsã€‚
    æ•°æ®è½¬æ¢ï¼šå­˜å‚¨å›¾åƒçš„é¢„å¤„ç†æ–¹æ³• transformã€‚
__getitem__ æ–¹æ³•ï¼š
    æ ¹æ®ç´¢å¼•è·å–å›¾åƒè·¯å¾„å’Œå¯¹åº”çš„æ–‡æœ¬ token IDã€‚
    å°è¯•åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒï¼Œå¤±è´¥æ—¶è¿”å›å…¨é›¶å¼ é‡ã€‚
__len__ æ–¹æ³•ï¼šè¿”å›æ•°æ®é›†çš„é•¿åº¦ã€‚
"""
# å›¾ç‰‡åç¼€ï¼Œ 
IMAGE_EXTENSIONS = [".png", ".jpg", ".jpeg", ".webp", ".bmp", ".PNG", ".JPG", ".JPEG", ".WEBP", ".BMP"]

class Text2ImageDataset(torch.utils.data.Database):
    """
    (1) ç›®æ ‡:
        - ç”¨äºæ„å»ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¾®è°ƒæ•°æ®é›†
    """

    def __init__(self, images_folder, captions_folder, transform, tokenizer):
        """
        (2) å‚æ•°:
            - images_folder: str, å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            - captions_folder: str, æ ‡æ³¨æ–‡ä»¶å¤¹è·¯å¾„
            - transform: function, å°†åŸå§‹å›¾åƒè½¬æ¢ä¸º torch.Tensor
            - tokenizer: CLIPTokenizer, å°†æ–‡æœ¬æ ‡æ³¨è½¬ä¸º word ids
        """
        # åˆå§‹åŒ–å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„æ‰©å±•åæ‰¾åˆ°æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_paths = []
        for ext in IMAGE_EXTENSIONS:
            self.image_paths.extend(glob.glob(os.path.join(images_folder, f"*{ext}")))
        
        self.image_paths = sorted(self.image_paths)

        # åŠ è½½å¯¹åº”çš„æ–‡æœ¬æ ‡æ³¨ï¼Œä¾æ¬¡è¯»å–æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­çš„å†…å®¹
        caption_paths = sorted(glob.glob(os.path.join(captions_folder, "*.txt")))
        captions = []
        for p in caption_paths:
            with open(p, "r", encoding="utf-8") as f:
                captions.append(f.readline().strip())

        #ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸€è‡´
        if len(captions) != len(self.image_paths):
            raise ValueError("å›¾åƒæ•°é‡ä¸æ–‡æœ¬æ ‡æ³¨æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†ã€‚")
        
        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºword ids
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        self.input_ids = inputs.input_ids
        self.transform = transform

    def __getitem__(self, idx):
        img_path  = self.image_paths[idx]
        input_id = self.input_ids[idx]
        try: 
            # åŠ è½½å›¾åƒå¹¶å°†å…¶è½¬æ¢ä¸º RGB æ¨¡å¼ï¼Œç„¶ååº”ç”¨æ•°æ®å¢å¼º
            image = Image.open(img_path).convert("RGB")
            tensor = self.input_ids[idx]
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒè·¯å¾„: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªå…¨é›¶çš„å¼ é‡å’Œç©ºçš„è¾“å…¥ ID ä»¥é¿å…å´©æºƒ
            tensor = torch.zero((3, resolution, resolution))
            input_id = torch.zero_like(input_id)
        
        return tensor, input_id # è¿”å›å¤„ç†åçš„å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æ ‡æ³¨
    
    def __len__(self):
        return len(self.image_paths)
    
### å®šä¹‰å¾®è°ƒç›¸å…³çš„å‡½æ•° ------------------------------------------------------------------

def perpare_lora_model(lora_config, pretrained_model_name_or_path, model_path=None, resume=False, merge_lora=False):
    """
    (1) ç›®æ ‡:
        - åŠ è½½å®Œæ•´çš„ Stable Diffusion æ¨¡å‹ï¼ŒåŒ…æ‹¬ LoRA å±‚ï¼Œå¹¶æ ¹æ®éœ€è¦åˆå¹¶ LoRA æƒé‡ã€‚è¿™åŒ…æ‹¬ Tokenizerã€å™ªå£°è°ƒåº¦å™¨ã€UNetã€VAE å’Œæ–‡æœ¬ç¼–ç å™¨ã€‚

    (2) å‚æ•°:
        - lora_config: LoraConfig, LoRA çš„é…ç½®å¯¹è±¡
        - pretrained_model_name_or_path: str, Hugging Face ä¸Šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„
        - model_path: str, é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
        - resume: bool, æ˜¯å¦ä»ä¸Šä¸€æ¬¡è®­ç»ƒä¸­æ¢å¤
        - merge_lora: bool, æ˜¯å¦åœ¨æ¨ç†æ—¶åˆå¹¶ LoRA æƒé‡

    (3) è¿”å›:
        - tokenizer: CLIPTokenizer
        - noise_scheduler: DDPMScheduler
        - unet: UNet2DConditionModel
        - vae: AutoencoderKL
        - text_encoder: CLIPTextModel

    åŠ è½½æ¨¡å‹ç»„ä»¶ï¼š ä¾æ¬¡åŠ è½½äº†å™ªå£°è°ƒåº¦å™¨ã€Tokenizerã€æ–‡æœ¬ç¼–ç å™¨ï¼ˆtext_encoderï¼‰ã€VAE å’Œ UNet æ¨¡å‹ã€‚
    åº”ç”¨ LoRAï¼š ä½¿ç”¨ get_peft_model å‡½æ•°å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œ unet æ¨¡å‹ä¸­ã€‚è¿™ä¼šåœ¨æ¨¡å‹ä¸­æ’å…¥å¯è®­ç»ƒçš„ LoRA å±‚ã€‚
    æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼š è°ƒç”¨ print_trainable_parameters() æ¥æŸ¥çœ‹ LoRA æ·»åŠ äº†å¤šå°‘å¯è®­ç»ƒå‚æ•°ã€‚
    æ¢å¤è®­ç»ƒï¼š å¦‚æœè®¾ç½®äº† resume=Trueï¼Œåˆ™ä»æŒ‡å®šçš„ model_path åŠ è½½ä¹‹å‰ä¿å­˜çš„æ¨¡å‹æƒé‡ã€‚  
    åˆå¹¶ LoRA æƒé‡ï¼š å¦‚æœ merge_lora=Trueï¼Œåˆ™å°† LoRA çš„æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶ä½¿ç”¨ï¼Œæ„Ÿå…´è¶£çš„è¯é˜…è¯»ï¼š14. PEFTï¼šåœ¨å¤§æ¨¡å‹ä¸­å¿«é€Ÿåº”ç”¨ LoRAã€‚
    å†»ç»“ VAE å‚æ•°ï¼š è°ƒç”¨ vae.requires_grad_(False) æ¥å†»ç»“ VAE çš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒä¸­ä¸æ›´æ–°ã€‚
    ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼š å°†æ‰€æœ‰æ¨¡å‹ç»„ä»¶ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ï¼Œå¹¶è®¾ç½®æ•°æ®ç±»å‹ã€‚
    """
    # åŠ è½½å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºæ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å™ªå£°æ·»åŠ å’Œç§»é™¤è¿‡ç¨‹
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")

    # åŠ è½½ Tokenizerï¼Œ ç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸º tokens
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # åŠ è½½ CLIP æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œ ç”¨äºå°†æ–‡æœ¬æ ‡æ³¨è½¬æ¢ä¸ºç‰¹å¾å‘é‡
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dytpe=weight_dtype,
        subfolder="text_encoder"
    )

    # åŠ è½½ VAE æ¨¡å‹
    vae = AutoencoderKL.from_trained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )

    # åŠ è½½ UNet æ¨¡å‹ï¼Œ è´Ÿè´£å¤„ç†æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒç”Ÿæˆå’Œæ¨ç†è¿‡ç¨‹
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )

    # å¦‚æœè®¾ç½®ä¸ºç»§ç»­è®­ç»ƒï¼Œåˆ™åŠ è½½ä¸Šä¸€æ¬¡çš„æ¨¡å‹æƒé‡
    if resume:
        if model_path is None or not os.path.exists(model_path):
            raise ValueError("å½“ resume è®¾ç½®ä¸ºtrueæ—¶ï¼Œ å¿…é¡»æä¾›æœ‰æ•ˆçš„ model_path")
        # ä½¿ç”¨ PEFT çš„from_pretrained æ–¹æ³•åŠ è½½ LoRA æ¨¡å‹
        text_encoder = PeftModel.from_pretrained(text_encoder,  os.path.join(model_path, "text_encoder"))
        unet = PeftModel.from_pretrained(unet, os.path.join(model_path), "unet")

        # ç¡®ä¿ UNet çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸ºTrue
        for param in unet.parameters():
            if param.requires_grad is False:
                param.requires_grad = True
        
        # ç¡®ä¿æ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°çš„ requires_grad ä¸º True
        for param in text_encoder.parameters():
            if param.requires_grad is False:
                param.requires_grad = True

        print("f"âœ… å·²ä» {model_path} æ¢å¤æ¨¡å‹æƒé‡"")

    else: 
        # å°† LoRA é…ç½®åº”ç”¨åˆ° text_encoder å’Œunet
        text_encoder = get_peft_model(text_encoder, lora_config)
        unet = get_peft_model(unet, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        print("ğŸ“Š Text Encoder å¯è®­ç»ƒå‚æ•°:")
        text_encoder.print_trainable_parameters()
        print("ğŸ“Š UNet å¯è®­ç»ƒå‚æ•°:")
        unet.print_trainable_parameters()

    if merge_lora:
        # åˆå¹¶LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œ ä»…åœ¨æ¨ç†æ—¶è°ƒç”¨
        text_encoder = text_encoder.merge_and_unload()
        unet = unet.merge_and_unload()

        # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
        text_encoder.eval()
        unet.eval()

    # å†»ç»“ VAE å‚æ•°
    vae.requires_grad_(False)

    # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU ä¸Šå¹¶è®¾ç½®æƒçš„æ•°æ®ç±»å‹
    unet.to(DEVICE, dtype=weight_dtype)
    vae.to(DEIVCE, dtype=weight_dtype)
    text_encoder.to(DEIVCE, dtype=weight_dtype)

    return tokenizer, noise_scheduler, unet, vae, text_encoder

### å‡†å¤‡ä¼˜åŒ–å™¨  ------------------------------------------------------------------
def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):
    """
    (1) ç›®æ ‡:
        - ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å¯è®­ç»ƒå‚æ•°åˆ†åˆ«è®¾ç½®ä¼˜åŒ–å™¨ï¼Œå¹¶æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚

    (2) å‚æ•°:
        - unet: UNet2DConditionModel, Hugging Face çš„ UNet æ¨¡å‹
        - text_encoder: CLIPTextModel, Hugging Face çš„æ–‡æœ¬ç¼–ç å™¨
        - unet_learning_rate: float, UNet çš„å­¦ä¹ ç‡
        - text_encoder_learning_rate: float, æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡

    (3) è¿”å›:
        - è¾“å‡º: ä¼˜åŒ–å™¨ Optimizer
    """
    # ç­›é€‰å‡º UNet ä¸­éœ€è¦è®­ç»ƒçš„ Lora å±‚å‚æ•°
    unet_lora_layers = [p for p in unet.parameters() if p.requires_grad]

    # ç­›é€‰å‡ºæ–‡æœ¬ç¼–ç å™¨ä¸­éœ€è¦è®­ç»ƒçš„ Loraå±‚å‚æ•°
    text_encoder_lora_layers = [p for p in text_encoder.parameters() if p.requires_grad]

    # å°†éœ€è¦è®­ç»ƒçš„å‚æ•°åˆ†ç»„å¹¶è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    trainable_params = [
        {"params": unet_lora_layers, "lr": unet_learning_rate},
        {"params": text_encoder_lora_layers, "lr": text_encoder_learning_rate}
    ]

    # ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainable_params)

    return optimizer

### å®šä¹‰ collate_fn å‡½æ•°ï¼Œ å°†æ‰¹æ¬¡æ•°æ®ç»„ç»‡æˆå­—å…¸çš„å½¢å¼ï¼Œæ–¹ä¾¿é€šè¿‡é”®åç›´æ¥è®¿é—®ï¼Œä¾‹å¦‚ batch["pixel_values"] å’Œ batch["input_ids"]ã€‚
def collate_fn(examples):
    pixel_values = []
    input_ids = []
    
    for tensor, input_id in examples:
        pixel_values.append(tensor)
        input_ids.append(input_id)
    
    pixel_values = torch.stack(pixel_values, dim=0).float()
    input_ids = torch.stack(input_ids, dim=0)
    
    # å¦‚æœä½ å–œæ¬¢åˆ—è¡¨æ¨å¯¼å¼çš„è¯ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•
    #pixel_values = torch.stack([example[0] for example in examples], dim=0).float()
    #input_ids = torch.stack([example[1] for example in examples], dim=0)
    return {"pixel_values": pixel_values, "input_ids": input_ids}


import torch
from torch.utils.data import DataLoader, Dataset

def compare_dataloaders(dataset, batch_size):
    # ç¬¬ä¸€ç§æƒ…å†µï¼šä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fn
    train_dataloader_custom = DataLoader(
       dataset,
       shuffle=True,
       collate_fn=collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fn
       batch_size=batch_size,
    )
    
    # ç¬¬äºŒç§æƒ…å†µï¼šä¸ä½¿ç”¨è‡ªå®šä¹‰çš„ collate_fnï¼ˆé»˜è®¤æ–¹å¼ï¼‰
    train_dataloader_default = DataLoader(
       dataset,
       shuffle=True,
       batch_size=batch_size,
    )
    
    # ä»æ¯ä¸ªæ•°æ®åŠ è½½å™¨ä¸­å–ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œå¯¹æ¯”
    custom_batch = next(iter(train_dataloader_custom))
    default_batch = next(iter(train_dataloader_default))
    
    # æ‰“å°è‡ªå®šä¹‰ collate_fn çš„è¾“å‡ºç»“æœ
    print("ä½¿ç”¨è‡ªå®šä¹‰ collate_fn:")
    print("æ‰¹æ¬¡çš„ç±»å‹:", type(custom_batch))
    print("æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶:", custom_batch["pixel_values"].shape)
    print("æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶:", custom_batch["input_ids"].shape)
    
    # æ‰“å°é»˜è®¤ DataLoader çš„è¾“å‡ºç»“æœ
    print("\nä½¿ç”¨é»˜è®¤ collate_fn:")
    print("æ‰¹æ¬¡çš„ç±»å‹:", type(default_batch))
    
    pixel_values, input_ids = default_batch
    print("æ‰¹æ¬¡ pixel_values çš„å½¢çŠ¶:", pixel_values.shape)
    print("æ‰¹æ¬¡ input_ids çš„å½¢çŠ¶:", input_ids.shape)
    
    return custom_batch, default_batch
       
# å¯¹æ¯”
custom_batch, default_batch = compare_dataloaders(dataset, batch_size=2)

###è®¾ç½®ç›¸å…³çš„å‚æ•° -------------------------------------------------------------------
#è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# For Mac M1, M2...
# DEVICE = torch.device("mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu"))

print(f"ğŸ–¥ å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")

# æ¨¡å‹ä¸è®­ç»ƒå‚æ•°é…ç½®
'''
è®­ç»ƒå‚æ•°ï¼šè®¾ç½®æ‰¹æ¬¡å¤§å°ã€æ•°æ®ç±»å‹ã€éšæœºç§å­ç­‰ã€‚
train_batch_size = 2 æ—¶ï¼Œå¾®è°ƒæ˜¾å­˜è¦æ±‚ä¸º 5Gï¼Œåœ¨å‘½ä»¤è¡Œè¾“å…¥ nvidia-smi å¯ä»¥æŸ¥çœ‹å½“å‰æ˜¾å­˜å ç”¨ã€‚
ä¼˜åŒ–å™¨å‚æ•°ï¼šä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è®¾ç½®å­¦ä¹ ç‡ã€‚
å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šé€‰æ‹© cosine_with_restarts è°ƒåº¦å™¨ï¼Œè¿™ä¸€ç‚¹ä¸€èˆ¬æ— å…³ç´§è¦ã€‚
é¢„è®­ç»ƒæ¨¡å‹ï¼šæŒ‡å®šé¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ã€‚
LoRA é…ç½®ï¼šè®¾ç½® LoRA çš„ç›¸å…³å‚æ•°ï¼Œå¦‚ç§© rã€lora_alphaã€åº”ç”¨æ¨¡å—ç­‰ã€‚
'''

# è®­ç»ƒç›¸å…³å‚æ•°
train_batch_size = 2  # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œå³æ¯æ¬¡è®­ç»ƒä¸­å¤„ç†çš„æ ·æœ¬æ•°é‡
weight_dtype = torch.bfloat16  # æƒé‡æ•°æ®ç±»å‹ï¼Œä½¿ç”¨ bfloat16 ä»¥èŠ‚çœå†…å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦
snr_gamma = 5  # SNR å‚æ•°ï¼Œç”¨äºä¿¡å™ªæ¯”åŠ æƒæŸå¤±çš„è°ƒèŠ‚ç³»æ•°

# è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
seed = 1126 # éšæœºç§å­
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Stable Diffusion LoRA çš„å¾®è°ƒå‚æ•°

# ä¼˜åŒ–å™¨å‚æ•°
unet_learning_rate = 1e-4 # UNet çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶ UNet å‚æ•°æ›´æ–°çš„æ­¥é•¿
text_encoder_learning_rate = 1e-4  # æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–‡æœ¬åµŒå…¥å±‚çš„å‚æ•°æ›´æ–°æ­¥é•¿

# å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
lr_scheduler_name = "cosine_with_restarts"  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸º Cosine annealing with restartsï¼Œé€æ¸å‡å°‘å­¦ä¹ ç‡å¹¶å®šæœŸé‡å¯
lr_warmup_steps = 100  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œåœ¨æœ€åˆçš„ 100 æ­¥ä¸­é€æ¸å¢åŠ å­¦ä¹ ç‡åˆ°æœ€å¤§å€¼
max_train_steps = 2000  # æ€»è®­ç»ƒæ­¥æ•°ï¼Œå†³å®šäº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„è¿­ä»£æ¬¡æ•°
num_cycles = 3  # Cosine è°ƒåº¦å™¨çš„å‘¨æœŸæ•°é‡ï¼Œåœ¨è®­ç»ƒæœŸé—´ä¼šé‡å¤ 3 æ¬¡å­¦ä¹ ç‡å‘¨æœŸæ€§é€’å‡å¹¶é‡å¯

# é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ
pretrained_model_name_or_path = "stablediffusionapi/cyberrealistic-41"  

# LoRA é…ç½®
lora_config = LoraConfig(
    r=32, # LoRA çš„ç§©ï¼Œå³ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œå†³å®šäº†å‚æ•°è°ƒæ•´çš„è‡ªç”±åº¦
    lora_alpha=16, # ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA æƒé‡å¯¹æ¨¡å‹çš„å½±å“
    target_modules=[
        "q_proj", "v_proj", "k_proj", "out_proj",  # æŒ‡å®š Text encoder çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŠ•å½±çŸ©é˜µï¼‰
        "to_k", "to_q", "to_v", "to_out.0"  # æŒ‡å®š UNet çš„ LoRA åº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´ UNet ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰
    ],
    lora_dropout=0 # LoRA dropout æ¦‚ç‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ dropout
)


### å¾®è°ƒå‰çš„å‡†å¤‡ ------------------------------------------------------------------
# å‡†å¤‡æ•°æ®é›†
# åˆå§‹åŒ– tokenizer
tokenizer = CLITokenizer.from_pretrained(
    pretrained_model_name_or_path,
    subfolder="tokenizer"
)

# å‡†å¤‡æ•°æ®é›†
dataset = Text2ImageDataset(
    images_folder=images_folder,
    captions_folder=captions_folder,
    transform=train_transform,
    tokenizer=tokenizer,
)

train_dataloader = torch.utils.data.DataLoader(
    dataset,
    shuffle=True,
    collate_fn=collate_fn,  # ä¹‹å‰å®šä¹‰çš„collate_fn()
    batch_size=train_batch_size,
    num_workers=8,
)
print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")

### å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨ --------------------------------------------------------------
'''
å‡†å¤‡æ¨¡å‹ï¼š è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ prepare_lora_model å‡½æ•°ã€‚
å‡†å¤‡ä¼˜åŒ–å™¨ï¼š è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ prepare_optimizer å‡½æ•°ã€‚
è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š ä½¿ç”¨ Hugging Face çš„ get_scheduler å‡½æ•°ã€‚
'''
# å‡†å¤‡æ¨¡å‹
tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(
    lora_config,
    pretrained_model_name_or_path,
    model_path,
    resume=False,
    merge_lora=False
)

# å‡†å¤‡ä¼˜åŒ–å™¨
optimizer = prepare_optimizer(
    unet, 
    text_encoder, 
    unet_learning_rate=unet_learning_rate, 
    text_encoder_learning_rate=text_encoder_learning_rate
)

# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
lr_scheduler = get_scheduler(
    lr_scheduler_name,
    optimizer=optimizer,
    num_warmup_steps=lr_warmup_steps,
    num_training_steps=max_train_steps,
    num_cycles=num_cycles
)

print("âœ… æ¨¡å‹å’Œä¼˜åŒ–å™¨å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")


### å¼€å§‹å¾®è°ƒ ---------------------------------------------------------------------
'''
- è®­ç»ƒå¾ªç¯ï¼š æˆ‘ä»¬åœ¨å¤šä¸ª epoch ä¸­è¿›è¡Œè®­ç»ƒï¼Œç›´åˆ°è¾¾åˆ° max_train_stepsã€‚æ¯ä¸ª epoch ä»£è¡¨ä¸€è½®æ•°æ®çš„å®Œæ•´è®­ç»ƒï¼Œåœ¨å¸¸è§çš„ UI ç•Œé¢ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ° epoch å’Œ max_train_steps çš„å‚æ•°ã€‚
- ç¼–ç å›¾åƒï¼š ä½¿ç”¨ VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatent spaceï¼‰ï¼Œä»¥ä¾¿åç»­åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ·»åŠ å™ªå£°å¹¶è¿›è¡Œå¤„ç†ã€‚
- æ·»åŠ å™ªå£°ï¼š ä½¿ç”¨å™ªå£°è°ƒåº¦å™¨ï¼ˆnoise_schedulerï¼‰ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ éšæœºå™ªå£°ï¼Œæ¨¡æ‹Ÿå›¾åƒä»æ¸…æ™°åˆ°å™ªå£°çš„é€€åŒ–è¿‡ç¨‹ã€‚è¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å…³é”®æ­¥éª¤ï¼Œè®­ç»ƒæ—¶æ¨¡å‹é€šè¿‡å­¦ä¹ å¦‚ä½•è¿˜åŸå™ªå£°ï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆæ¸…æ™°çš„å›¾åƒã€‚
- è·å–æ–‡æœ¬åµŒå…¥ï¼š ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨ï¼ˆtext_encoderï¼‰å°†è¾“å…¥çš„æ–‡æœ¬ prompt è½¬æ¢ä¸ºéšè—çŠ¶æ€ï¼ˆæˆ‘ä»¬è§è¿‡å¾ˆå¤šç±»ä¼¼çš„è¡¨è¾¾ï¼šéšè—å‘é‡/ç‰¹å¾å‘é‡/embedding/...ï¼‰ï¼Œä¸ºå›¾åƒç”Ÿæˆæä¾›æ–‡æœ¬å¼•å¯¼ä¿¡æ¯ã€‚
- è®¡ç®—ç›®æ ‡å€¼ï¼š æ ¹æ®æ‰©æ•£æ¨¡å‹çš„ç±»å‹ï¼ˆepsilon æˆ– v_predictionï¼‰ï¼Œç¡®å®šæ¨¡å‹çš„ç›®æ ‡è¾“å‡ºï¼ˆå™ªå£°æˆ–é€Ÿåº¦å‘é‡ï¼‰ã€‚
- UNet é¢„æµ‹ï¼š ä½¿ç”¨ UNet æ¨¡å‹å¯¹å¸¦å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºè¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆçš„è¾“å‡ºç”¨äºè¿˜åŸå™ªå£°æˆ–é¢„æµ‹é€Ÿåº¦å‘é‡ã€‚
- è®¡ç®—æŸå¤±ï¼š é€šè¿‡åŠ æƒå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è®¡ç®—æ¨¡å‹æŸå¤±ï¼Œå¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚
- ä¼˜åŒ–ä¸ä¿å­˜ï¼šé€šè¿‡ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨é€‚å½“æ—¶ä¿å­˜æ£€æŸ¥ç‚¹ã€‚
'''

# ç¦ç”¨å¹¶è¡ŒåŒ–ï¼Œ é¿å…è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ– 
global_step = 0
best_face_score = float("inf") # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§ï¼Œ å­˜å‚¨æœ€ä½³é¢éƒ¨ç›¸ä¼¼åº¦åˆ†æ•°

# è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
progress_bar = tqdm(
    range(max_train_steps), # æ ¹æ® num_training_steps è®¾ç½®
    desc="è®­ç»ƒæ­¥éª¤"
)

# è®­ç»ƒå¾ªç¯
for epoch in range(math.ceil(max_train_steps / len(train_dataloader))):
    # å¦‚æœä½ æƒ³åœ¨è®­ç»ƒä¸­å¢åŠ è¯„ä¼°ï¼Œé‚£åœ¨å¾ªç¯ä¸­å¢åŠ train() æ˜¯æœ‰å¿…è¦çš„
    unet.train()
    text_encoder.train()

    for step, batch in enumerate(train_dataloader):
        if global_step >= max_train_steps:
            break

        # ç¼–ç å›¾åƒä¸ºæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰
        latents = vae.encode(batch["pixel_values"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()
        latents = latents * vae.config.scaling.scaling_factor  # æ ¹æ® VAE çš„ç¼©æ”¾å› å­è°ƒæ•´æ½œåœ¨ç©ºé—´

        # ä¸ºæ½œåœ¨è¡¨ç¤ºæ·»åŠ å™ªå£°ï¼Œ ç”Ÿæˆå¸¦å™ªå£°çš„å›¾åƒ
        noise = torch.ran_like(latents)  # ç”Ÿæˆä¸æ½œåœ¨è¡¨ç¤ºç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

         # è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤º
        encoder_hidden_states = text_encoder(batch["input_ids"].to(DEVICE))[0]

        # è®¡ç®—ç›®æ ‡å€¼
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise  # é¢„æµ‹å™ªå£°
        elif noise_scheduler.config.prediction_type == "v_prediction":
            target = noise_scheduler.get_velocity(latents, noise, timesteps)  # é¢„æµ‹é€Ÿåº¦å‘é‡

        # UNet æ¨¡å‹é¢„æµ‹
        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)[0]

        # è®¡ç®—æŸå¤±
        if not snr_gamma:
            loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
        else:
            # è®¡ç®—ä¿¡å™ªæ¯” (SNR) å¹¶æ ¹æ® SNR åŠ æƒ MSE æŸå¤±
            snr = compute_snr(noise_scheduler, timesteps)
            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]
            if noise_scheduler.config.prediction_type == "epsilon":
                mse_loss_weights = mse_loss_weights / snr
            elif noise_scheduler.config.prediction_type == "v_prediction":
                mse_loss_weights = mse_loss_weights / (snr + 1)
            
            # è®¡ç®—åŠ æƒçš„ MSE æŸå¤±
            loss = F.mse_loss(model_pred.float(), target.float(), reduction="none")
            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
            loss = loss.mean()

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
        global_step += 1

        # æ‰“å°è®­ç»ƒæŸå¤±
        if global_step % 100 == 0 or global_step == max_train_steps:
            print(f"ğŸ”¥ æ­¥éª¤ {global_step}, æŸå¤±: {loss.item()}")

        # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œå½“å‰ç®€å•è®¾ç½®ä¸ºæ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡
        if global_step % 500 == 0:
            save_path = os.path.join(output_folder, f"checkpoint-{global_step}")
            os.makedirs(save_path, exist_ok=True)

            # ä½¿ç”¨ save_pretrained ä¿å­˜ PeftModel
            unet.save_pretrained(os.path.join(save_path, "unet"))
            text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
            print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´æ¨¡å‹åˆ° {save_path}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° checkpoint-last
save_path = os.path.join(output_folder, "checkpoint-last")
os.makedirs(save_path, exist_ok=True)
unet.save_pretrained(os.path.join(save_path, "unet"))
text_encoder.save_pretrained(os.path.join(save_path, "text_encoder"))
print(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")

print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")