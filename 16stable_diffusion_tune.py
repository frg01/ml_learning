'''
pip install timm
pip install fairscale
pip install transformers
pip install requests
pip install accelerate
pip install diffusers
pip install einop
pip install safetensors
pip install voluptuous
pip install jax
pip install jaxlib
pip install peft
pip install deepface==0.0.92
pip install tensorflow==2.9.0  # ä¸ºäº†é¿å…æœ€åè¯„ä¼°é˜¶æ®µä½¿ç”¨deepfaceæ—¶çš„é”™è¯¯ï¼Œè¿™é‡Œé€‰æ‹©é™çº§ç‰ˆæœ¬
pip install keras
pip install opencv-python
'''
### å¯¼å…¥ ---------------------------------------------------------------
# ========== æ ‡å‡†åº“æ¨¡å— ==========
import os
import math
import glob
import shutil
import subprocess

# ========== ç¬¬ä¸‰æ–¹åº“ ==========
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm

# ========== æ·±åº¦å­¦ä¹ ç›¸å…³åº“ ==========
from torchvision import transforms

# Transformers (Hugging Face)
from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor

# Diffusers (Hugging Face)
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    UNet2DConditionModel,
    DiffusionPipeline
)
from diffusers.optimization import get_scheduler
from diffusers.training_utils import compute_snr

# ========== LoRA æ¨¡å‹åº“ ==========
from peft import LoraConfig, get_peft_model, PeftModel

# ========== é¢éƒ¨æ£€æµ‹åº“ ==========
from deepface import DeepFace

import cv2


### è®¾ç½®é¡¹ç›®è·¯å¾„ ------------------------------------------------------------
# é¡¹ç›®åç§°å’Œæ•°æ®é›†åç§°
project_name = "Brad"
dataset_name = "Brad"

# æ ¹ç›®å½•å’Œä¸»è¦ç›®å½•
root_dir = "./"  # å½“å‰ç›®å½•
main_dir = os.path.join(root_dir, "SD")  # ä¸»ç›®å½•

# é¡¹ç›®ç›®å½•
project_dir = os.path.join(main_dir, project_name)  # é¡¹ç›®ç›®å½•

# æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„
images_folder = os.path.join(main_dir, "Datasets", dataset_name)
prompts_folder = os.path.join(main_dir, "Datasets", "prompts")
captions_folder = images_folder  # ä¸åŸå§‹ä»£ç ä¸€è‡´
output_folder = os.path.join(project_dir, "logs")  # å­˜æ”¾ model checkpoints å’Œ validation çš„æ–‡ä»¶å¤¹

# prompt æ–‡ä»¶è·¯å¾„
validation_prompt_name = "validation_prompt.txt"
validation_prompt_path = os.path.join(prompts_folder, validation_prompt_name)

# æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
model_path = os.path.join(project_dir, "logs", "checkpoint-last")

# å…¶ä»–è·¯å¾„è®¾ç½®
zip_file = os.path.join("./", "data/14/Datasets.zip")
inference_path = os.path.join(project_dir, "inference")  # ä¿å­˜æ¨ç†ç»“æœçš„æ–‡ä»¶å¤¹

os.makedirs(images_folder, exist_ok=True)
os.makedirs(prompts_folder, exist_ok=True)
os.makedirs(output_folder, exist_ok=True)
os.makedirs(inference_path, exist_ok=True)

# æ£€æŸ¥å¹¶è§£å‹æ•°æ®é›†
print("ğŸ“‚ æ­£åœ¨æ£€æŸ¥å¹¶è§£å‹æ ·ä¾‹æ•°æ®é›†...")

if not os.path.exists(zip_file):
    print("âŒ æœªæ‰¾åˆ°æ•°æ®é›†å‹ç¼©æ–‡ä»¶ Datasets.zipï¼")
    print("è¯·ä¸‹è½½æ•°æ®é›†:\n../Demos/data/14/Datasets.zip\nå¹¶æ”¾åœ¨ ./data/14 æ–‡ä»¶å¤¹ä¸‹")
else:
    subprocess.run(f"unzip -q -o {zip_file} -d {main_dir}", shell=True)
    print(f"âœ… é¡¹ç›® {project_name} å·²å‡†å¤‡å¥½ï¼")


### å¯¼å…¥æ•°æ® -----------------------------------------------------------
# è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡
resolution = 512

# æ•°æ®å¢å¼ºæ“ä½œ
train_transform = transforms.Compose(
    [
        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),  # è°ƒæ•´å›¾åƒå¤§å°
        transforms.CenterCrop(resolution),  # ä¸­å¿ƒè£å‰ªå›¾åƒ
        transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡
    ]
)

