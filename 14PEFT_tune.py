# PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ Hugging Face æä¾›çš„ä¸“é—¨ç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒçš„å·¥å…·åº“ã€‚

'''å¾®è°ƒæ–¹æ³•
Prefix-Tuningï¼šå†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œä¸ºæ¯ä¸€å±‚æ·»åŠ å¯å­¦ä¹ çš„å‰ç¼€å‘é‡ï¼Œåªå­¦ä¹ å‰ç¼€å‚æ•°ã€‚
Adapter-Tuningï¼šå†»ç»“åŸæ¨¡å‹å‚æ•°ï¼Œåœ¨æ¨¡å‹çš„å±‚ä¸å±‚ä¹‹é—´æ’å…¥å°å‹çš„ adapter æ¨¡å—ï¼Œä»…å¯¹ adapter æ¨¡å—è¿›è¡Œè®­ç»ƒã€‚
ç­‰ã€‚ã€‚ã€‚
'''

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ --------------------------------------------------------------
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 çš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('opus-mt-zh-en')
model = AutoModelForCausalLM.from_pretrained('opus-mt-zh-en')

# print(model)


### åº”ç”¨LoRA --------------------------------------------------------------------
from peft import get_peft_model, LoraConfig, TaskType

# é…ç½®LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹
    inference_mode=False,         # æ¨ç†æ¨¡å¼å…³é—­ï¼Œä»¥è¿›è¡Œè®­ç»ƒ
    r=8,                          # ä½ç§©å€¼r
    lora_alpha=32,                # LoRA çš„ç¼©æ”¾å› å­
    lora_dropout=0.1,             # Dropout æ¦‚ç‡
    target_modules=["q_proj", "v_proj"],  # ğŸ’¡ å…³é”®å‚æ•°
)

# å°† LoRA åº”ç”¨åˆ°æ¨¡å‹ä¸­
model = get_peft_model(model, lora_config)
# print(model)

# æŸ¥çœ‹ LoRA æ¨¡å—
model.print_trainable_parameters()


def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        all_params += num_params
        if param.requires_grad:
            trainable_params += num_params
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")
    print(f"æ€»å‚æ•°é‡: {all_params}")
    print(f"å¯è®­ç»ƒå‚æ•°å æ¯”: {100 * trainable_params / all_params:.2f}%")
    
# print_trainable_parameters(model)


### å‡†å¤‡æ•°æ®è¿›è¡Œå¾®è°ƒ --------------------------------------------------------------------
from transformers import Trainer, TrainingArguments
# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',         # æ¨¡å‹ä¿å­˜å’Œæ—¥å¿—è¾“å‡ºçš„ç›®å½•è·¯å¾„
    num_train_epochs=3,             # è®­ç»ƒçš„æ€»è½®æ•°ï¼ˆepochsï¼‰
    per_device_train_batch_size=16, # æ¯ä¸ªè®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œ16è¡¨ç¤ºæ¯æ¬¡è¾“å…¥æ¨¡å‹çš„æ•°æ®æ•°é‡
    learning_rate=5e-5,             # å­¦ä¹ ç‡
    logging_steps=10,               # æ¯éš”å¤šå°‘æ­¥ï¼ˆstepsï¼‰è¿›è¡Œä¸€æ¬¡æ—¥å¿—è®°å½•
    save_steps=100,                 # æ¯éš”å¤šå°‘æ­¥ä¿å­˜æ¨¡å‹
)

from datasets import Dataset
# å‡è®¾æˆ‘ä»¬æƒ³è®­ç»ƒä¸­ç¿»è‹±
raw_data = [
    {"translation": {"zh": "ä½ å¥½", "en": "Hello"}},
    {"translation": {"zh": "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "en": "The weather is nice today"}},
]

# ç¼–ç ä¸ºæ¨¡å‹è®­ç»ƒè¾“å…¥æ ¼å¼ï¼ˆå›  CausalLM åªçœ‹ input_idsï¼‰
def preprocess(example):
    input_text = example["translation"]["zh"]
    target_text = example["translation"]["en"]
    combined = input_text + " </s> " + target_text  # ä½ ä¹Ÿå¯ä»¥åªè¾“å…¥ source
    tokenized = tokenizer(combined, truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

train_dataset = Dataset.from_list(raw_data).map(preprocess)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,                    # è®­ç»ƒçš„æ¨¡å‹å¯¹è±¡ï¼Œéœ€è¦äº‹å…ˆåŠ è½½å¥½
    args=training_args,             # ä¸Šé¢å®šä¹‰çš„è®­ç»ƒå‚æ•°é…ç½®
    train_dataset=train_dataset,    # éœ€è¦å¯¹åº”æ›¿æ¢æˆå·²ç»å¤„ç†è¿‡çš„dataset
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

### ä¿å­˜å’ŒåŠ è½½LoRA å¾®è°ƒçš„æ¨¡å‹ ----------------------------------------------------------
#ä¿å­˜LoRAå‚æ•°
model.save_pretrained('./lora_model')

# åŠ è½½åŸå§‹æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("opus-mt-zh-en")

# åŠ è½½LoRA å‚æ•°
from peft import PeftModel
model = PeftModel.from_pretrained(base_model, './lora_model')


### åˆå¹¶LoRA å¹¶å¸è½½PEFTåŒ…è£…ï¼ˆå‡å°‘ä¾èµ–æˆä¸ºæ ‡å‡†æ¨¡å‹ã€æé«˜æ¨ç†æ•ˆç‡ã€ç®€åŒ–æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ï¼‰------------
# å¯¹æ¯”åˆå¹¶å‰åçš„æ¨¡å‹
print("åˆå¹¶å‰çš„æ¨¡å‹ç»“æ„ï¼š")
print(model)

# åˆå¹¶ï¼Œå¹¶å¸è½½LoRAæƒé‡
model = model.merge_and_unload()

print("åˆå¹¶åçš„æ¨¡å‹ç»“æ„")
print(model)


# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
model.save_pretrained('./merged_model')
tokenizer.save_pretrained('./merged_model')