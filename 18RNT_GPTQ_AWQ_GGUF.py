"""
| 方法/格式       | 类型     | 主要用途      | 精度    | 推理效率 | 适用场景           |
| ----------- | ------ | --------- | ----- | ---- | -------------- |
| RTN         | 简单离线量化 | 快速压缩      | 中低    | 高    | 原型/调试          |
| GPTQ        | 后训练量化  | 精度保持好     | 高     | 中    | 本地部署、微服务       |
| AWQ         | 激活感知量化 | 速度快       | 中高    | 中    | Web部署、Edge     |
| GGUF (GGML) | 推理格式   | 高效 CPU 运行 | 取决于量化 | 高    | PC/Mobile/离线部署 |

GPTQ(训练后量化)：模型的权重位宽降低到 3-4 位，大幅降低模型大小和计算成本的同时还能保持模型性能
AWQ（激活感知权重量化）:通过通道缩放保护显着权重。利用激活激活分配找到重要的权重并缩放它们的值来减少量化精度。
GGUF (GGML): 是一种文件格式，支持在 CPU 和 GPU 上进行推理,GGUF 是 GGML 的升级版本，提升了扩展和兼容性。

量化评价标准：
PPL(Perplexity)困惑度：
较低的 PPL：困惑度低，预测下一个词汇越优秀，更准确地预测了文本中的词汇，对语言结构的掌握更好。

新的量化方法：k-quants（k-bit Quantization）：精确到k位比特数值的量化。
| 格式名      | 精度（比特）       | 类型      | 特点说明                                  |
| -------- | ------------ | ------- | ------------------------------------- |
| `F16`    | 16-bit float | 半精度浮点   | 精度高，推理速度慢，占用大                         |
| `Q6_K`   | 6-bit        | 对称/块量化  | 精度最好的一种量化方案（在不考虑 FP16 的情况下）           |
| `Q5_K_S` | 5-bit        | 对称/块量化  | 精度略低于 Q6\_K，但推理更快，内存占用更少              |
| `Q4_K_S` | 4-bit        | 对称/块量化  | 相较于 `Q4_0`、`Q4_1` 有精度提升，适用于主流 CPU/GPU |
| `Q3_K_M` | 3-bit        | 对称/混合量化 | 极致压缩，精度进一步下降，适用于极低资源设备                |
| `Q2_K`   | 2-bit        | 对称/块量化  | 推理最快，精度最低                             |
| `Q8_0`   | 8-bit        | 对称      | 精度和内存介于 F16 与 Q6 之间，适合边缘设备            |

我的经验法则是：
- Q2和Q3通常只是用于娱乐、测试或实验
- Q4 是获得合理结果的最低标准，即第一个“实际”的量化位宽
- Q5 是我日常使用的最低标准，我不会在日常使用中选择比这个不同的量化
- Q6我不常用，或者用Q5，因为模型恰好能够适应我的系统；或者用Q8，因为它能够轻松适应我的系统
- Q8 是最优选
"""

